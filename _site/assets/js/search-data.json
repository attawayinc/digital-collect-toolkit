{
  "0": {
    "id": "0",
    "title": "",
    "content": "404 Page not found :( The requested page could not be found.",
    "url": "http://localhost:4000/digital-collect-toolkit/404.html",
    "relUrl": "/404.html"
  },
  "1": {
    "id": "1",
    "title": "About the Toolkit",
    "content": "About the Toolkit As the events of Friday August 11th and Saturday August 12th, 2017 in Charlottesville unfolded, Library staff at the University of Virginia actively watched the news and began capturing information from websites and social media. When Library administration met the following Tuesday morning, we were asked if we could create a site that allowed community members to contribute their photos and videos online. While the UVA Library had some experience documenting and collecting digital content after a major news event, this was the first time we attempted to create a collecting site so quickly after the events occurred. We consulted with other institutions who had created similar sites, and built on their previous work. We set up workflows to ensure that no malware or viruses were upload in the process. It took use a little over three weeks to launch our online collecting tool, far longer than we had initially hoped. The lessons learned from the site launch of the University of Virginia Library’s Digital Collecting site, “Unite the Right” Rally and Community Response has led to further workflow and tool development to help future collecting efforts for ourselves and others. This Toolkit is a result of these efforts, and is intended to help others quickly launch similar sites in times of crisis. If you have any questions or comments, please feel free to contact us at digital_collecting@virginia.edu. Visit our digital collection site:",
    "url": "http://localhost:4000/digital-collect-toolkit/about/",
    "relUrl": "/about/"
  },
  "2": {
    "id": "2",
    "title": "Additional Resources",
    "content": "Additional Resources for Social Media Data Collection Twarc Hydrator Tweet ID Datasets Catalog Twarc UNLV Libraries’ Twitter Data Tutorial Series, which features ten tutorials that take you step-by-step through the design, collection, and documentation process of curating a collection of Twitter data, as well as tutorials on tools for data analysis. This tutorial series focuses on using Twarc for twitter data collection. Hydrator Hydrator is a downloadable app for your computer developed by DocNow for hydrating Twitter ID datasets. Hydrator does not require a Twitter developer account or registered app, and requires no coding knowledge. It streamlines the process of linking your Twitter account, hydrating tweet ID lists, and exporting the raw JSON data into csv for easy access to the dataset. This tool is great for researchers who want to do a quick check of the contents of a dataset in Excel - the csv export function in the Hydrator makes it easy to build a spreadsheet of tweets. For access to a step-by-step guide to installing, configuring and using the Hydrator, click here. Tweet ID Datasets Catalog DocNow maintains a catalog of open-access, dehydrated Twitter datasets, with information for each including size, collection date, topic tags and a brief description. This is a great place to start if you want to explore the range of Twitter data currently being collected by scholars active in the field. To browse the catalogue, click here.",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/social-media/additional-resources.html",
    "relUrl": "/docs/social-media/additional-resources.html"
  },
  "3": {
    "id": "3",
    "title": "Twarc Introductory Lesson",
    "content": "Introductory Lesson on Twarc for Twitter Data Collection Why Collect Tweets? Data Collection Process Common Twarc Collection Methods Used in Archiving Search Filter Sample Timeline Side by Side: Search vs Filter Dehydrated and Rehydrated Data Sets Start Collecting: Twarc Command Basics Collecting Tweets Dehydrate your Dataset Rehydrate a Dataset Why Collect Tweets? Twitter is undeniably a part of the cultural landscape of the modern world, and its content represents a new form of the historical record, one that archivists around the world are actively working to preserve. Particularly relevant to researchers interested in exploring popular movements, the dynamics of fast-moving socio-political events, and the digital footprint of contemporary culture, Twitter archiving is becoming increasingly more prominent in the catalog of activities taking place under the umbrella of digital curation. Like other digital archiving efforts, the collection and curation of Twitter data does involve some unique challenges, not least of which is the functionally infinite size of the dataset. For this reason, a large part of collecting, archiving, and providing access to Twitter data involves deciding what to collect and why. Any Twitter archive will reflect the intention of its collector - from the tools used to gather the data to the kind of search used to collect, Twitter archives are always a directed slice of the ever-expanding pie that is social media. Data Collection Process The University of Virginia currently archives collections of Twitter data using Twarc, a command line tool and Python library developed as part of the Documenting the Now project. Twarc provides several different methods for collecting Twitter data, which are outlined below. Note that each collection method has a distinct goal in mind, and will have slightly different outputs depending on how the data is collected. It is important to note the kind of collection process used to generate a specific Twitter archive - the table below outlines some of these variations and the effects on their output. The type of collection method should be noted in the description of each Twitter archive, as well as the date and size of the collection, along with the dates of collection and dataset size. Common Twarc Collection Methods Used in Archiving For more detailed usage documentation, see the following page on Twarc Commands. You can also skip ahead to the beginner’s tutorial on using Twarc. Search Collects pre-existing tweets from up to seven days ago that match the given query. This method allows you to gather tweets that are already published, but will not collect any tweets that have been made private or deleted by the time you run your search. Filter Initiates a collection process that will gather tweets matching the given query as they are published. This process does not gather tweets that were already published by the time the filter is initialized, but will ensure a high-fidelity capture of all material between the start and end times of the filter. Unlike a search, it is also not limited to a single week, and can be run as long as you want. Sample This method returns a random sampling of tweets. Timeline The timeline command will collect the most recent tweets from a single user. Like search, it can only go back seven days. Side by Side: Search vs Filter The following example illustrates some of the differences between using search and filter methods with the same arguments and output. In this case, we are looking for all tweets containing the keyword “charlottesville”.   Search Filter Twarc Command twarc search charlottesville &gt; charlottesville-tweets.jsonl twarc filter charlottesville &gt; charlottesville-tweets.jsonl Description Searches all existing tweets from the present to seven days earlier (this time limit is set by Twitter). Initiates a collection process that will gather tweets, as they are published, until you tell it to stop. Output Returns results as a JSONL file, where each JSON object = a single tweet. Returns results as a JSONL file, where each JSON object = a single tweet. Notes Captures all relevant tweets up to a week prior to the search date, but will not capture any tweets that have been either privatized or deleted by the time the search is run. A higher-fidelity collecting method that archives tweets in real time. Twarc filtering requires a dedicated machine that stays on for the duration of the process. The blue text in the twarc command field is modifiable: this is where you tell Twarc what you want to search for, and where you want to name the JSONL file that will contain the results. Dehydrated and Rehydrated Data Sets Twitter API’s Terms of Service does not allow for making large amounts of raw Twitter data available on the Web. The fully-hydrated Twitter data you collect using Twarc can be used for research and archived for local use, but cannot be shared publically. However, Twitter does allow files of tweet identifiers to be publically shared. This is useful for when you would like to make a dataset of tweets available, for example, in DocNow’s Tweet Catalogue. This is referred to as a “dehydrated” data set - each tweet is reduced to its unique ID number, and a list of these IDs is saved as a text document. You can use Twarc to dehydrate your data set. A dehydrated data set can be “rehydrated” using Twarc or another program of your choice - Twitter will take each ID and search the current Twitterverse for a corresponding tweet. If it locates a tweet that matches the ID, it will return the original JSON code for that tweet. This process is called “rehydration”, and will return a JSON file containing the data for every tweet that it was able to locate. It’s important to note that any tweet rehydrator can only check for tweets that are currently live - they cannot find tweets that have been deleted or made private since the original collection event. Start Collecting: Twarc Command Basics Collecting Tweets Let’s start collecting tweets! This introductory lesson will use Twarc’s search command to return tweets containing ‘BlackLivesMatter’ occurring within the past 7 days. For further instructions on using other commands, see the following page on Twarc Commands. This lesson assumes some basic familiarity with the command line (Terminal for Mac users, PowerShell for Windows users). Create a new folder on your desktop titled ‘BlackLivesMatter_Tweets’. Open Terminal (for Mac) or PowerShell (for Windows) Change directories to navigate into the ‘BlackLivesMatter_Tweets’ folder by typing into the command line cd followed by the path of your folder: cd desktop/BlackLivesMatter_Tweets Hit ‘return’ to complete the command. You can also navigate to your folder by typing cd followed by a space, then drag your folder into the Terminal or PowerShell window. Hit return to complete the command To check if you are in the correct place, use the command pwd to display the path of your current directory. It should end with /BlackLivesMatter_Tweets Now that you are in the right directory, enter the following command to start collecting tweets: twarc search blacklivesmatter &gt; blacklivesmatter_tweets.jsonl Tip: You can copy and paste these commands into Terminal or PowerShell to avoid errors Note: Your collection may take some time to return all Tweets. You can tell when the process is complete when it returns to the shell ($) (or PowerShell (PS) ) prompt: While the prompt is running, check to make sure your command was successful by clicking on your ‘BlackLivesMatter_Tweets’ folder. Inside you should see your ‘blacklivesmatter_tweets.jsonl’ file and a ‘twarc.log’ file. For the purposes of this tutorial, you can cut the search short. After entering the initial search command, wait 5 minutes and then enter Ctrl + C to stop the search. Note: If you wanted to collect the full set of tweets, you would wait until the process was finished. You can tell when a process is complete when it returns to the shell ($) prompt. You’re done! Your tweets are in JSON format in your ‘blacklivesmatter_tweets.jsonl’ file. Dehydrate your Dataset Each Tweet in your dataset has a unique identifier. Twarc’s dehydrate command will generate a list of tweet ids from a file of tweets. This lesson will show you how to dehydrate your ‘blacklivesmatter_tweets.jsonl’ file so that you can share your dataset while keeping to Twitter API’s Terms of Service. Navigate into you ‘BlackLivesMatter_Tweets’ folder using the cd command, or if you are already there, check you directory location using the pwd command. Enter the command ls to list all files in the directory. You should see the files ‘blacklivesmatter_tweets.jsonl’ and ‘twarc.log’. It will look something like this: &#39;your-computer-name&#39;:BlackLivesMatter_Tweets &#39;your-user-name&#39;$ ls blacklivesmatter_tweets.jsonl twarc.log To dehydrate your tweets, enter the following command: twarc dehydrate blacklivesmatter_tweets.jsonl &gt; blacklivesmatter_tweet_ids.txt You should now have a text file containing the unique tweet ids of all tweets in your dataset. Your tweet ids are located in your ‘BlackLivesMatter_Tweets’ folder in the ‘blacklivesmatter_tweet_ids.txt’ file. Open the blacklivesmatter_tweet_ids.txt file to see the list of unique tweet ids: Rehydrate a Dataset Twarc’s hydrate command will read your file of unique identifiers and write out the tweet JSON for them using Twitter’s status/lookup API. This is useful if you have a set of Twitter ids from another institution and would like to view the full dataset. Documenting the Now has a collection of Tweet ids that you can explore and rehydrate here, but for this tutorial we will use the ‘blacklivesmatter_tweet_ids.txt’ file you created when you dehydrated your dataset. Navigate into you ‘BlackLivesMatter_Tweets’ folder using the cd command, or if you are already there, check you directory location using the pwd command. Rehydrate your dataset by entering the following command: twarc hydrate blacklivesmatter_tweet_ids.txt &gt; blacklivesmatter_tweets_hydrated.jsonl You now have your tweets in JSON format ready to go in your ‘BlackLivesMatter_Tweets’ folder. Check your folder to confirm your .jsonl file is there. Now you’re ready for more complex Twarc commands that will allow you to create a collection to fit your research needs: Twarc Commands.",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/social-media/collect-tweets.html",
    "relUrl": "/docs/social-media/collect-tweets.html"
  },
  "4": {
    "id": "4",
    "title": "Collecting Materials",
    "content": "Collecting Materials We’ve included detailed documentation on how our Digital Collecting site uses the Contribution plugin to collect stories, images, videos, and links from the public. Please read through the Installation &amp; Configuration Guide for full set-up instructions before getting started. Creating Contribution Types Contributor Anonymity Settings Contribution Terms of Service Creating Contribution Types After installing the plugin, find the settings for Contribution by clicking on the “Contributed Items” tab on the left-hand navigation panel of the Omeka admin dashboard. Here you will see four tabs: Getting Started, Contribution Types, Submission Settings, and Contributions. Select “Contribution Types” to manage the types of items you’d like users to share through the form: See in the above image that we have included four contribution types for our site: Story, Photograph, Link, and Video. The type’s Name is the label that will appear in your site’s Contribution Form: The Item Type is the type of object associated with your Contribution Type, and is selected when you add a new contribution type to your form (see the green button ‘Add a Type’ on the right). These are not visible to the public, but can help in defining search fields. Omeka comes with pre-defined item types, found by clicking on the “Item Types” tab on the left-hand navigation panel of the Omeka admin dashboard. You can edit and add to this list if needed (see the related Omeka Documentation). Our site uses the pre-defined types: Text, Still Image, Hyperlink, and Moving Image. Contributor Anonymity Settings Under the tab for “Submission Settings” you can set options for contributor anonymity. Our site allows both Non-Registered Contributions and Anonymous Contributions: It is important to make clear in your Terms of Service that user anonymity is conditional, and it is likely that all submissions and associated data may be provided to federal, state, or local law enforcement or other government agencies pursuant to a lawful subpoena or court order. See our Terms of Service section for more details. For further documentation on Contributution Submission Settings and Contributor anonymity settings, see the related Omeka Documentation. Contribution Terms of Service We offer a general Contribution Terms of Service template for others as a model. Our terms were written in collaboration with University of Virgina General Council. We recommend reviewing your terms with relevant parties for your own collecting site. Contribution Terms of Service: You are being asked to contribute your recollections, photographic images, video, social media postings or other digital content to [insert institution here], which is creating a digital record of the [describe event, location, and date(s) here]. You may only submit material created entirely by you and not copied from or based, in whole or in part, upon any other photographic, literary, or other material, except to the extent that such material is in the public domain, or you have permission of the copyright owner, or its use is allowed by “Fair Use” as prescribed by the terms of United States copyright law. If you would like to refer or nominate material which you do not own, please contact us at [insert email address or link to google nominating form]. You must be 18 years of age or older to submit material. By submitting content through this form, you are granting [insert institution here] permission to disseminate, preserve, and use that content in connection with its educational and research mission, including promotional purposes, in all media in perpetuity. You retain ownership of and copyright in the material you share. If you indicate on the form that your submission is “public,” your material may be published on the web (with or without your name, depending on what you have indicated) as part of [insert institution here] digital collections or exhibits. Otherwise, your material will only be available to [insert institution here]-approved researchers. Submitted material must not violate any confidentiality, privacy, security or other laws. Please be aware that all submissions and any information associated with the submissions (email address, descriptive information, etc.) may be provided to federal, state, or local law enforcement or other government agencies pursuant to a lawful subpoena or otherwise as required by law. We reserve the right to discard or mark private any submission that [institution] staff identify as irrelevant or for any other reason within their professional judgment. [Insert institution here] is not obligated to include your content in this project or preserve it in perpetuity. In additon, our Digital Collecting site’s Terms of Service includes a Summary of Terms to help contributors quickly read and understand the main points of the full terms, in simpler language. It is important to note to users that a summary does not replace the full terms, and submissions are governed by the full terms of use. Summary of Terms* You must be at least 18 years old. Submitted material must be owned and/or created by you. You have the option of making your contribution public or private. If public, your content may be published as part of the Library’s digital collections (with or without your name displayed, depending on what you have indicated). All submissions will be available to Library-approved researchers and can be used by the Library from now on in support of its teaching and research mission. Your submission must not violate any laws. If we receive a lawful subpoena or court order, we may be required to turn over any submissions and related information (email address, descriptive information, etc.). *This summary is to help you read and understand the terms, but does not replace them. Your submission is governed by the full terms of use.",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/omeka-setup/collecting-materials.html",
    "relUrl": "/docs/omeka-setup/collecting-materials.html"
  },
  "5": {
    "id": "5",
    "title": "Custom Styling",
    "content": "Custom Styling for your Site",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/omeka-setup/custom-styling.html",
    "relUrl": "/docs/omeka-setup/custom-styling.html"
  },
  "6": {
    "id": "6",
    "title": "Custom Omeka Theme",
    "content": "Custom Omeka Theme Our Digital Collecting site uses a custom Omeka Theme: Charlotesville Rally Theme. Steps for Installing Our Custom Theme Download and unzip our custom theme from Github. Download the .zip file to your desktop for easy retrieval. Locate your Omeka installation and login to access your site. Navigate to your Omeka folder (this should have the same name as your Omeka install) Open your Omeka folder and locate the ‘/themes’ folder within. Copy or move the unzipped ‘cville_rally_theme’ folder from your desktop (or from where you saved this folder) and place it withing the ‘omeka/themes’ folder located in step #4. Log in to your Omeka admin panel (found at the url: ‘your-site-url/admin’), and click on the ‘Appearance’ option in the top naviation bar. The custom theme should now be installed and visible (see screenshot below). If not, double-check that the folder is in the right location (‘/themes’) and that the folder name for the theme does not start with ‘theme-‘. Click on “Configure Theme” to customize your site’s appearance. See Custom Styling for further details on customization. For more detailed instructions on installing an Omeka theme, see the related Omeka Documentation.",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/omeka-setup/custom-theme.html",
    "relUrl": "/docs/omeka-setup/custom-theme.html"
  },
  "7": {
    "id": "7",
    "title": "",
    "content": "Welcome to the Digital Collecting Toolkit! What is this Toolkit? This toolkit is designed to provide resources and instruction on implementing digital collecting strategies during and after rapidly evolving social events and/or community crises (like campus controversies, natural disasters and public emergencies). Photos, videos, and social media content are major components of these community experiences, and the tools offered here can help organizations, institutions and communities quickly implement an effective digital collecting initiative. This toolkit was created through a collaborative process by a team at the University of Virgina Library, with funding from the LYRASIS Catalyst Fund. In the immediate aftermath of the events of August 11th and 12th in Charlottesvile, VA, UVa Library staff took on the task of quickly launching an online collecting tool and capturing related social media content. While the UVA Library had some experience documenting and collecting digital content after a major news event, this was the first time we attempted to create a collecting site so quickly after the events occurred. The lessons learned from the site launch of the University of Virginia Library’s Digital Collecting site, “Unite the Right” Rally and Community Response has led to further workflow and tool development to help future collecting efforts for ourselves and others. This Toolkit is a result of these efforts, and is intended to help others quickly launch similar sites in times of crisis. Who is this for? Our team includes University librarians, digital preservation specialists, archivists, digital content developers, and IT specialists. We developed this toolkit for use by a wide range of cultural institutions and communities with an interest in quickly setting up a digital collection site. Members of University libraries, other educational institutions, and community organizations using this toolkit will need to have some level of access to the tools provided here to implement a digital collection strategy. This could include access to web hosting, server space, and an Omeka installation. However, we have included a range of potential options for these requirements in this toolkit. How do I get started? This toolkit is designed so you can get started quickly! Read the basics on getting your collection site up and running. See the steps for setting up an Omeka Collection Site. See the section on Social Media Tools to learn how to set up and use the DocNow tool, Twarc for collecting Twitter data. Contact us If you have any questions or comments, you can reach us at digital_collecting@virginia.edu.",
    "url": "http://localhost:4000/digital-collect-toolkit/",
    "relUrl": "/"
  },
  "8": {
    "id": "8",
    "title": "Getting Started",
    "content": "Getting Started with Omeka Classic",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/omeka-setup/installing-omeka.html",
    "relUrl": "/docs/omeka-setup/installing-omeka.html"
  },
  "9": {
    "id": "9",
    "title": "Install & Configure Twarc",
    "content": "Installing and Configuring Twarc Before you Begin Twarc for Mac OS Installing Twarc Configuring Twarc Twarc for Windows Installing Twarc Configuring Twarc Before you Begin Before using Twarc, you will need to register a Twitter application, and have your consumer key, consumer secret, access token, and access token secret on hand. Twarc requires familiarity with using the command line to navigate your file system, configure Twarc, and run queries. For introductory lessons on using the command line, see these tutorials for Mac and Windows users from the UNLV Libraries, here. The Command Line tutorial is part of UNLV Libraries’ Twitter Data Tutorial Series, which features ten tutorials that take you step-by-step through the design, collection, and documentation process of curating a collection of Twitter data, as well as tutorials on tools for data analysis. This series is focused on using Twarc and offers another resource, in addition to this toolkit. Download and install the latest version of Python, here. Twarc works with with either version 2 or 3. If you are a Mac user, you may already have Python installed. To check, open your terminal and run the command: python -V or python --version Twarc for Mac OS For Windows users, click here Installing Twarc Open the Terminal application (located in the applications folder) Pip install Twarc by entering the following command: pip install twarc Note: Pip is already installed if you are using Python 2 ≥ 2.7.9 or Python 3 ≥ 3.4 macOS users also have the option of installing Twarc via homebrew using the command: brew install twarc Configuring Twarc To get started, you will need to tell Twarc about your application API keys and grant access to one or more Twitter accounts. Follow these directions to configure Twarc: Enter the following command in Terminal: twarc configure Twarc will ask you to enter several keys. You should have these keys ready to go after registering an application. Type or copy/paste your consumer key, then press ‘enter’. Next, Twarc will ask for your consumer secret. Type or copy/paste your consumer secret, then press ‘enter’. Next, Twarc will ask you to log into your twitter account. Copy/paste the provided URL into your browser to authorize your application. Following the provided URL will bring you to a page that looks something like this: Click ‘Authorize app’. A pin number will be provided, Type this pin into your terminal, as seen in step 3, then press ‘enter’. After entering your pin, your terminal should give a message telling you where your twitter credentials are saved, followed by “Happy twarcing!” This means your twarc installation is configured. Next, learn the basics of colleting twitter data: Twarc Introductory Lesson Twarc for Windows Installing Twarc Open PowerShell (to open PowerShell, use the taskbar to search for PowerShell and select ‘Windows PowerShell’) Pip install Twarc by entering the following command: pip install twarc Note: Pip is already installed if you are using Python 2 ≥ 2.7.9 or Python 3 ≥ 3.4 Configuring Twarc To get started, you will need to tell Twarc about your application API keys and grant access to one or more Twitter accounts. Follow these directions to configure Twarc: Enter the following command in PowerShell: twarc configure Twarc will ask you to enter several keys. You should have these keys ready to go after registering an application. Type or copy/paste your consumer key, then press ‘enter’. Next, Twarc will ask for your consumer secret. Type or copy/paste your consumer secret, then press ‘enter’. Next, learn the basics of colleting twitter data: Twarc Introductory Lesson",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/social-media/installing-twarc.html",
    "relUrl": "/docs/social-media/installing-twarc.html"
  },
  "10": {
    "id": "10",
    "title": "Omeka Plugins",
    "content": "Omeka Plugins Our Digital Collecting site includes the following plugins. Links to download for your site are included below, as well as details on how we used the plugins. Please follow the links for related installation and user guides for details on how to set-up and use the individual plugins. Guest User Contribution Simple Vocab Dublin Core Extended Element Types Derivative Images Simple Pages Guest User You must upload and install the Guest User plugin before installing and activating the Contribution Plugin. Download Guest User plugin Installation and Configuration Guide Contribution Download Contribution plugin Installation &amp; Configuration Guide See Collecting Materials for more information on collecting and managing stories, images, and other files from the public. Simple Vocab Download Simple Vocab plugin Configuration &amp; Use Guide Simple Vocab is a useful plugin for providing predetermined vocabulary for any metadata elements. This plugin allows you to create drop-down menus that replace the usual text box for an element. We used Simple Vocab to replace the element for “Spatial Coverage” with a list of locations around Charlottesville relevant to the events: Dublin Core Extended Download Dublin Core Extended plugin Documentation Element Types Download Element Types plugin Used for standardizing date input for contribution submissions (provides a pop out calendar for date selection in the Contribution form) Derivative Images Download Derivative Images plugin Documentation Simple Pages Download Simple Pages plugin Configuration Guide Used to create our About the Archive page. This plugin is useful for creating additional text rich pages for your site, and comes bundled with your Omeka download.",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/omeka-setup/omeka-plugins.html",
    "relUrl": "/docs/omeka-setup/omeka-plugins.html"
  },
  "11": {
    "id": "11",
    "title": "Omeka Classic Setup",
    "content": "Setting Up Omeka Classic The University of Virginia Library’s Digital Collecting site, “Unite the Right” Rally and Community Response runs on Omeka Classic.",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/omeka-setup",
    "relUrl": "/docs/omeka-setup"
  },
  "12": {
    "id": "12",
    "title": "Building a Digital Collection",
    "content": "Building a Digital Collection Site Where to Start Gathering a Team Technical Specifications Developing a Site Site Hosting Reclaim Hosting Omeka.net Omeka Classic vs Omeka S Virus Scanning File Sizes Where to Start Gathering a Team Technical Specifications Developing a Site Site Hosting Reclaim Hosting Omeka.net Omeka Classic vs Omeka S Virus Scanning File Sizes",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/site-building/",
    "relUrl": "/docs/site-building/"
  },
  "13": {
    "id": "13",
    "title": "Social Media Tools",
    "content": "Tools for Archiving Social Media This guide presents methods of collecting twitter data using tools built by DocNow, a collaborative effort between Shift Design, Inc., the University of Maryland, and the University of Virginia, with funding from the Andrew W. Mellon Foundation. We use the tools developed by DocNow for collecting twitter data because of their strong commitment to prioritizing ethical practices in collection, use, and preservation of social media content. The following documentation is intended to assist in setting up Twarc, a DocNow tool for archiving twitter data. For more information on Twarc or other DocNow tools, please visit their site. Twarc for Twitter Data Collection Twarc is a command line tool that downloads tweets using Twitter’s API. API’s, or application programming interfaces, are simply ways that different organizations, whether it is Twitter or the Census bureau, provide more direct access to data. API’s also oftentimes provide limits to how much data you can gather. Twitter’s API limit for downloading tweets from a user account as of writing this is 3200 tweets. Twarc will handle Twitter API’s rate limits for you. The following pages provide instructions on installing and using Twarc. Parts of this guide are subject to change with updates to Twitter’s developers site, so please use this guide as a general guideline. For troubleshooting with Twarc, please contact the developers of DocNow, and join in conversation with the DocNow community of scholars, students, and archivists. To get started you will need a twitter account to register a twitter application. Twarc also requires Python.",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/social-media",
    "relUrl": "/docs/social-media"
  },
  "14": {
    "id": "14",
    "title": "Twarc Commands",
    "content": "Twarc Commands After you’ve become familiar with the basics of using Twarc to perform a search command to collect Twitter data, dehydrate an existing dataset, and hydrate a list of unique Tweet ids, you can move on to more complex commands to tailor your search to your research needs. Below you will find a list of commands that will allow you to create a more targeted collection. Search Filter Sample Dehydrate Hydrate Users Followers Friends Trends Timeline Retweets Replies Lists Search This uses Twitter’s search/tweets to download pre-existing tweets matching a given query. twarc search blacklivesmatter &gt; tweets.jsonl It’s important to note that search will return tweets that are found within a 7 day window that Twitter’s search API imposes. If this seems like a small window, it is, but you may be interested in collecting tweets as they happen using the filter and sample commands below. The best way to get familiar with Twitter’s search syntax is to experiment with Twitter’s Advanced Search and copy and pasting the resulting query from the search box. For example here is a more complicated query that searches for tweets containing either the #blacklivesmatter or #blm hashtags that were sent to deray. twarc search &#39;#blacklivesmatter OR #blm to:deray&#39; &gt; tweets.jsonl Twitter attempts to code the language of a tweet, and you can limit your search to a particular language if you want: twarc search &#39;#blacklivesmatter&#39; --lang fr &gt; tweets.jsonl You can also search for tweets with a given location, for example tweets mentioning blacklivesmatter that are 1 mile from the center of Ferguson, Missouri: twarc search blacklivesmatter --geocode 38.7442,-90.3054,1mi &gt; tweets.jsonl If a search query isn’t supplied when using --geocode you will get all tweets relevant for that location and radius: twarc search --geocode 38.7442,-90.3054,1mi &gt; tweets.jsonl Filter The filter command will use Twitter’s statuses/filter API to collect tweets as they happen. twarc filter blacklivesmatter,blm &gt; tweets.jsonl Please note that the syntax for the Twitter’s track queries is slightly different than what queries in their search API. So please consult the documentation on how best to express the filter option you are using. Use the follow command line argument if you would like to collect tweets from a given user id as they happen. This includes retweets. For example this will collect tweets and retweets from CNN: twarc filter --follow 759251 &gt; tweets.jsonl You can also collect tweets using a bounding box. Note: the leading dash needs to be escaped in the bounding box or else it will be interpreted as a command line argument! twarc filter --locations &quot; -74,40,-73,41&quot; &gt; tweets.jsonl If you combine options they are OR’ed together. For example this will collect tweets that use the blacklivesmatter or blm hashtags and also tweets from user CNN: twarc filter blacklivesmatter,blm --follow 759251 &gt; tweets.jsonl Sample Use the sample command to listen to Twitter’s statuses/sample API for a “random” sample of recent public statuses. twarc sample &gt; tweets.jsonl Dehydrate The dehydrate command generates an id list from a file of tweets: twarc dehydrate tweets.jsonl &gt; tweet-ids.txt Hydrate Twarc’s hydrate command will read a file of tweet identifiers and write out the tweet JSON for them using Twitter’s status/lookup API. twarc hydrate ids.txt &gt; tweets.jsonl Twitter API’s Terms of Service discourage people from making large amounts of raw Twitter data available on the Web. The data can be used for research and archived for local use, but not shared with the world. Twitter does allow files of tweet identifiers to be shared, which can be useful when you would like to make a dataset of tweets available. You can then use Twitter’s API to hydrate the data, or to retrieve the full JSON for each identifier. This is particularly important for verification of social media research. Users The users command will return User metadata for the given screen names. twarc users deray,Nettaaaaaaaa &gt; users.jsonl You can also give it user ids: twarc users 1232134,1413213 &gt; users.jsonl If you want you can also use a file of user ids, which can be useful if you are using the followers and friends commands below: twarc users ids.txt &gt; users.jsonl Followers The followers command will use Twitter’s follower id API to collect the follower user ids for exactly one user screen name per request as specified as an argument: twarc followers deray &gt; follower_ids.txt The result will include exactly one user id per line. The response order is reverse chronological, or most recent followers first. Friends Like the followers command, the friends command will use Twitter’s friend id API to collect the friend user ids for exactly one user screen name per request as specified as an argument: twarc friends deray &gt; friend_ids.txt Trends The trends command lets you retrieve information from Twitter’s API about trending hashtags. You need to supply a Where On Earth identifier (woeid) to indicate what trends you are interested in. For example here’s how you can get the current trends for St Louis: twarc trends 2486982 Using a woeid of 1 will return trends for the entire planet: twarc trends 1 If you aren’t sure what to use as a woeid just omit it and you will get a list of all the places for which Twitter tracks trends: twarc trends If you have a geo-location you can use it instead of the woedid. twarc trends 39.9062,-79.4679 Behind the scenes twarc will lookup the location using Twitter’s trends/closest API to find the nearest woeid. Timeline The timeline command will use Twitter’s user timeline API to collect the most recent tweets posted by the user indicated by screen_name. twarc timeline deray &gt; tweets.jsonl You can also look up users using a user id: twarc timeline 12345 &gt; tweets.jsonl Retweets You can get retweets for a given tweet id like so: twarc retweets 824077910927691778 &gt; retweets.jsonl Replies Unfortunately Twitter’s API does not currently support getting replies to a tweet. So twarc approximates it by using the search API. Since the search API does not support getting tweets older than a week twarc can only get all the replies to a tweet that have been sent in the last week. If you want to get the replies to a given tweet you can: twarc replies 824077910927691778 &gt; replies.jsonl Using the --recursive option will also fetch replies to the replies as well as quotes. This can take a long time to complete for a large thread because of rate limiting by the search API. twarc replies 824077910927691778 --recursive Lists To get the users that are on a list you can use the list URL with the listmembers command: twarc listmembers https://twitter.com/edsu/lists/bots",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/social-media/twarc-commands.html",
    "relUrl": "/docs/social-media/twarc-commands.html"
  },
  "15": {
    "id": "15",
    "title": "Twitter Setup",
    "content": "Setting up your Twitter Account for Collecting Steps for Creating a Twitter Application Setting up a Twitter Developer Account Creating a Twitter App Accessing Keys and Tokens Steps for Creating a Twitter Application Please note that these instructions may be subject to change with updates to Twitter. If the following steps and screenshots do not match precisely, please use this as a general guide, or contact DocNow. If you do not have a twitter account, create one at twitter.com. A twitter account is required for access to twitter data. Log into you twitter account to set-up and and authorize a twitter application. A Twitter application will let you download twitter data using Python. Create an app at developer.twitter.com/en/apps Following the above link should bring you to this page: Click on ‘Create an App’ You may be prompted to create a Twitter developer account. Select ‘Apply’ and continue: Setting up a Twitter Developer Account If you already have a developer accout set up, skip to Creating a Twitter App. Select a user profile to associate with the developer account, and select ‘Continue’. In the ‘Account details’ section, select your request for access for either your organization or for personal use. Note: Selecting “for my own personal use” offers the simplest and quickest method of setting up a developer account. If you are following this guide for research or teaching purposes, an individual, personal account is suggested. Twitter users can only have a single developement account, and cannot change account types. An ‘Organization’ account differs in that it allows for additional twitter users to share access on a single dev account, however, this is mainly geared toward commercial use and is intended for development for premium APIs ($), and is outside the scope of this guide. For more, see Twitter’s FAQ. If you select “for my own personal use,” you will be prompted to fill in an Account Name and to select your Primary Country of Operation. Both are required. If you select “for my organization,” you are required to fill out the following: Organization Name (the name of your account) Legal Entity Name (may be the same as Organization Name) Organization Twitter @username Organization Primary Country of Operation Customer Location (select ‘Not Applicable(we do not have customers)’) Categorize your Organization (select ‘Academic’) Industries served (select Academic) In the next section on ‘Use Case Details’, answer several questions about your project and why you are building an app to gather twitter data. You can provide answers similar to the ones below if relevant to your project. Most importantly note that this is for academic purposes and that you will not be Tweeting, Retweeting, or liking content. If your project does involve analysis of twitter content, an answer for question #2 might look something like “Yes, my project will analyze tweets using text analysis, word clouds, word frequency, and word association using R.” Read and Accept the Twitter Terms of Service. Verify your Twitter Developement Account via the email associated with your Twitter account. Creating a Twitter App Once you have your Twitter Developer Account set up, you can register an application at developer.twitter.com/en/apps. Click ‘Create an App’ to begin: Fill out the required parts of the form for App Details. For the Website URL, you can simply put the URL for your twitter account, or any website you are affiliated with: Click ‘Create’. A pop-up may appear for reviewing the Twitter Developer Terms, click ‘Create’ to continue. You now have a registered Twitter app! You can edit any of these fields later from your Developer Account Apps page. Accessing Keys and Tokens From your Developer Account Apps page, find your app and click ‘Details’. Select the option for ‘Keys and Tokens’. On this page you will find your Consumer API keys. Under ‘Access token &amp; access token secret’, click ‘create’ to generate. Note down for use with Twarc these four alphanumeric values: Consumer API key Consumer API secret Access token Access token secret Next, get Twarc up and running: Installing and Configuring Twarc",
    "url": "http://localhost:4000/digital-collect-toolkit/docs/social-media/twitter-setup.html",
    "relUrl": "/docs/social-media/twitter-setup.html"
  }
  
}
